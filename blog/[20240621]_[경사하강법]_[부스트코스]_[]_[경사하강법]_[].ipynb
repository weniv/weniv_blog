{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjSFTJzCNeK8"
   },
   "source": [
    "# [코드리뷰 프로젝트] 2. 경사하강법 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO9rnzsWOHgC"
   },
   "source": [
    "## 1. 경사하강법\n",
    "- 선형 회귀 모델을 구현하기 위한 데이터를 확인하고, 알맞은 형태로 분리합니다.\n",
    "- 분리한 데이터를 활용해 train, weight, bias를 정의합니다.\n",
    "- 경사하강법 구현을 위한 학습 루프를 만들고, 100회 반복해 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3FJjaifHlir"
   },
   "source": [
    "### [TODO] 1_ 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kVQ6vBGLOwKh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.] (6,)\n",
      "[10. 20. 30. 40. 50. 60.] (6,)\n"
     ]
    }
   ],
   "source": [
    "# 미션 : 아래와 같은 출력 결과가 나타나도록 코드를 작성해보세요.\n",
    "\n",
    "##############################################\n",
    "#### 코드리뷰 2-1. 알맞은 코드를 직접 작성해보세요! ####\n",
    "##############################################\n",
    "\n",
    "# 프로그램 실행 예시\n",
    "# 출력 결과\n",
    "\n",
    "# print(x_train, x_train.shape)\n",
    "# [1. 2. 3. 4. 5. 6.] (6,)\n",
    "\n",
    "# print(y_train, y_train.shape)\n",
    "# [10. 20. 30. 40. 50. 60.] (6,)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[1., 2., 3., 4., 5., 6.],\n",
    "               [10., 20., 30., 40., 50., 60.]])\n",
    "\n",
    "## 코드시작 ##\n",
    "x_train = xy[0]    \n",
    "y_train = xy[1]   \n",
    "## 코드종료 ##\n",
    "\n",
    "print(x_train, x_train.shape)\n",
    "print(y_train, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B0Pe2-HOORP"
   },
   "source": [
    "### [TODO] 2_ train, weight, bias 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8DU2paKP-PN"
   },
   "outputs": [],
   "source": [
    "# 미션 : 아래와 같은 출력 결과가 나타나도록 코드를 작성해보세요.\n",
    "\n",
    "##############################################\n",
    "#### 코드리뷰 2-2. 알맞은 코드를 직접 작성해보세요! ####\n",
    "##############################################\n",
    "\n",
    "# 프로그램 실행 예시\n",
    "# 출력 결과\n",
    "\n",
    "# print(beta_gd, bias)\n",
    "# [0.53764546] [0.71495179]    # randoma을 사용해 숫자를 만들어냈기 때문에, 출력결과는 다를 수 있습니다. 형태 위주로 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u9ZhXkGlPuA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5488135] [0.71518937]\n"
     ]
    }
   ],
   "source": [
    "## 코드시작 ##\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "beta_gd = np.random.rand(1) \n",
    "bias = np.random.rand(1)\n",
    "\n",
    "## 코드종료 ##\n",
    "\n",
    "print(beta_gd, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i07Boy0YOYd5"
   },
   "source": [
    "### [TODO] 3_ 경사하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-WjLLs7eRkr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001 | Epoch    0/1000 | Cost: 1307.957160 | w: 0.830493 | b: 0.779917\n",
      "Learning Rate: 0.001 | Epoch  100/1000 | Cost: 3.342027 | w: 9.045288 | b: 2.594001\n",
      "Learning Rate: 0.001 | Epoch  200/1000 | Cost: 1.280321 | w: 9.385576 | b: 2.572516\n",
      "Learning Rate: 0.001 | Epoch  300/1000 | Cost: 1.187434 | w: 9.419477 | b: 2.483084\n",
      "Learning Rate: 0.001 | Epoch  400/1000 | Cost: 1.103833 | w: 9.440746 | b: 2.394191\n",
      "Learning Rate: 0.001 | Epoch  500/1000 | Cost: 1.026122 | w: 9.460809 | b: 2.308380\n",
      "Learning Rate: 0.001 | Epoch  600/1000 | Cost: 0.953882 | w: 9.480136 | b: 2.225641\n",
      "Learning Rate: 0.001 | Epoch  700/1000 | Cost: 0.886727 | w: 9.498769 | b: 2.145868\n",
      "Learning Rate: 0.001 | Epoch  800/1000 | Cost: 0.824301 | w: 9.516735 | b: 2.068954\n",
      "Learning Rate: 0.001 | Epoch  900/1000 | Cost: 0.766269 | w: 9.534057 | b: 1.994796\n",
      "Learning Rate: 0.01 | Epoch    0/1000 | Cost: 0.712323 | w: 9.552233 | b: 1.916978\n",
      "Learning Rate: 0.01 | Epoch  100/1000 | Cost: 0.342855 | w: 9.689352 | b: 1.329946\n",
      "Learning Rate: 0.01 | Epoch  200/1000 | Cost: 0.165023 | w: 9.784481 | b: 0.922680\n",
      "Learning Rate: 0.01 | Epoch  300/1000 | Cost: 0.079429 | w: 9.850479 | b: 0.640130\n",
      "Learning Rate: 0.01 | Epoch  400/1000 | Cost: 0.038231 | w: 9.896266 | b: 0.444104\n",
      "Learning Rate: 0.01 | Epoch  500/1000 | Cost: 0.018401 | w: 9.928032 | b: 0.308107\n",
      "Learning Rate: 0.01 | Epoch  600/1000 | Cost: 0.008857 | w: 9.950071 | b: 0.213756\n",
      "Learning Rate: 0.01 | Epoch  700/1000 | Cost: 0.004263 | w: 9.965361 | b: 0.148298\n",
      "Learning Rate: 0.01 | Epoch  800/1000 | Cost: 0.002052 | w: 9.975968 | b: 0.102885\n",
      "Learning Rate: 0.01 | Epoch  900/1000 | Cost: 0.000988 | w: 9.983327 | b: 0.071379\n",
      "Learning Rate: 0.1 | Epoch    0/1000 | Cost: 0.000475 | w: 9.988814 | b: 0.047888\n",
      "Learning Rate: 0.1 | Epoch  100/1000 | Cost: 1161959030793105827660106121576970190848.000000 | w: -18239491699872653312.000000 | b: -4260370478260048384.000000\n",
      "Learning Rate: 0.1 | Epoch  200/1000 | Cost: 265971321260730079682586492719234729484045680501217946847142154222755258597755214017868436552670214180831232.000000 | w: -275952751501026950619630394573241726933643249758240768.000000 | b: -64456892507471326841020271114124002699740669924081664.000000\n",
      "Learning Rate: 0.1 | Epoch  300/1000 | Cost: 60880583444403879351129907445237753110402595622714952131720667411013922182281760350462630633326723672038536090463990077169882519193472630507591779331670127101284441130218291200.000000 | w: -4175002369255673606033729610850298591935707675456392159853551003382908518277253861212160.000000 | b: -975194766023376676268552791450205634092719085863339766471044382406294680693599169937408.000000\n",
      "Learning Rate: 0.1 | Epoch  400/1000 | Cost: 13935507869653371836127199421737824970219370477061145396257200298540350226813529056193984108362616703726569771796760586655746693554554875337283829717106419342863993377614260247245230520406510843136557875152303760385045460656933025037632690716672.000000 | w: -63165323369589871314682597509705402579833274447846534076565146091560085830077451252655086319985818597195038110037133230080.000000 | b: -14754121625847123017277883425918074194734363022027054323420208907952794762531995937643508433931664992723803471729846648832.000000\n",
      "Learning Rate: 0.1 | Epoch  500/1000 | Cost: inf | w: -955654086753527745104485120362931344082968438237592647647910969800074837262547570421313133223713545980092035923761658967945352283774237731606880488150532096.000000 | b: -223221158003088709259325808376040362910170231953291644783169009576415108712505294481490593337207126772919086222084313639083945963170860319117930441719414784.000000\n",
      "Learning Rate: 0.1 | Epoch  600/1000 | Cost: inf | w: -14458482673871779128149189940749590582547108567430131866441425087126550435292313736327719737311679338071575979312670503347334995681912251803023348959747365140532126172014462936458169579208704.000000 | b: -3377204461494277797619216395305789925332570548233347308602935204494887371275406588200379313839873658575099261278593602713192033645397233252535446424499408755435159713272910778351927141859328.000000\n",
      "Learning Rate: 0.1 | Epoch  700/1000 | Cost: inf | w: -218748315031865547746044287417369712234033542163161396482820245146240936295290356072133532990302927331534308838354925715752824779437841333092949641381290208213111754057732716488968368331646986719770819885855190534609134682112.000000 | b: -51095111577994021388003123625013426359304244360699826056281727062641290415802448280003162264285160869361844308100048807144871993405315553178531668105587508011997708353967003591050406456956598274170178315582803032268745998336.000000\n",
      "Learning Rate: 0.1 | Epoch  800/1000 | Cost: inf | w: -3309532985487645103242085121534018392404649896621916300501432307593337164958376965827294323826632211868485179598965232895384904857987684630205444015822907562661850198338253555244827932059259691432043768885295512530530679693137020868000420005886343685792071680.000000 | b: -773038901533525170269429538928800677228180609449062072521079894438167430114401243954095262774193895722735902460034540954765146012409898419214188066940605065502344278280334732917286859353144702859126445023438173838087371419200245695756149488281055606612164608.000000\n",
      "Learning Rate: 0.1 | Epoch  900/1000 | Cost: inf | w: -50071282059636478959846158386425963260179356276438157711057642800190726033186146779127442171943197962809511285272963307178081400076636926516493256940671569700470647344787630354373062079196611336643670494903372213984677116205125468928941332196197862355840766202343684537752127129225393950687232.000000 | b: -11695622630590995298509016095596328031289085083485921745762438200130338909455051023864832443263229910628752817278215334548791744208392399012131110083460981804950544520808578658750596864804536481055783391983502647012105042274848815914722601412703616213826409060096854132215182216236158962106368.000000\n",
      "Learning Rate: 0.001 | Final Cost: 0.712843 | Final w: 9.550593 | Final b: 1.923999\n",
      "Learning Rate: 0.01 | Final Cost: 0.000479 | Final w: 9.988391 | Final b: 0.049702\n",
      "Learning Rate: 0.1 | Final Cost: nan | Final w: nan | Final b: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\defy2\\AppData\\Local\\Temp\\ipykernel_4124\\3257767725.py:25: RuntimeWarning: overflow encountered in square\n",
      "  error = ((pred - y_train) ** 2).mean()\n",
      "C:\\Users\\defy2\\AppData\\Local\\Temp\\ipykernel_4124\\3257767725.py:30: RuntimeWarning: invalid value encountered in subtract\n",
      "  beta_gd -= lr * gd_w\n"
     ]
    }
   ],
   "source": [
    "# 미션 : 위와 같은 출력 결과가 나타나도록 코드를 작성해보세요.\n",
    "\n",
    "##############################################\n",
    "#### 코드리뷰 2-3. 알맞은 코드를 직접 작성해보세요! ####\n",
    "##############################################\n",
    "\n",
    "np.random.seed(0)\n",
    "beta_gd = np.random.rand(1)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "# 다양한 학습률 설정\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "# 결과 저장\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # 파라미터 초기화\n",
    "    beta_gd = beta_gd.copy()\n",
    "    bias = bias.copy()\n",
    "    \n",
    "    # 학습\n",
    "    for i in range(1000):\n",
    "        pred = beta_gd * x_train + bias\n",
    "        error = ((pred - y_train) ** 2).mean()\n",
    "\n",
    "        gd_w = ((pred - y_train) * 2 * x_train).mean()\n",
    "        gd_b = ((pred - y_train) * 2).mean()\n",
    "\n",
    "        beta_gd -= lr * gd_w\n",
    "        bias -= lr * gd_b\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Learning Rate: {lr} | Epoch {i:4d}/1000 | Cost: {error:.6f} | w: {beta_gd.item():.6f} | b: {bias.item():.6f}')\n",
    "\n",
    "    # 최종 비용 저장\n",
    "    results.append((lr, error, beta_gd.item(), bias.item()))\n",
    "\n",
    "# 결과 출력\n",
    "for lr, final_error, final_beta, final_bias in results:\n",
    "    print(f'Learning Rate: {lr} | Final Cost: {final_error:.6f} | Final w: {final_beta:.6f} | Final b: {final_bias:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtXjewgoToQJ"
   },
   "source": [
    "ALL RIGHTS RESERVED. (C)NAVER Connect Foundation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPcb8y5G8e2x3/BYIZHACWq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
